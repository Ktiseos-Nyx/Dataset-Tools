[
    {
        "id": 6,
        "name": "CLIPTextEncode",
        "prompts": {
            "text": "Editing Instruction: Convert the image into a Ghibli style."
        }
    },
    {
        "id": 8,
        "name": "VAEDecode"
    },
    {
        "id": 9,
        "name": "SaveImage"
    },
    {
        "id": 13,
        "name": "LoadImage",
        "prompts": {
            "text": "pasted/image (96).png"
        }
    },
    {
        "id": 37,
        "name": "workflow>IP2PSampler"
    },
    {
        "id": 42,
        "name": "QuadrupleCLIPLoader",
        "prompts": {
            "text": "clip_g_hidream.safetensors",
            "text_2": "clip_l_hidream.safetensors"
        }
    },
    {
        "id": 43,
        "name": "VAELoader",
        "prompts": {
            "text": "vae.safetensors"
        }
    },
    {
        "id": 44,
        "name": "ImageScale",
        "prompts": {
            "text": "nearest-exact"
        }
    },
    {
        "id": 46,
        "name": "GeminiAPI",
        "prompts": {
            "text": "\"\"\"Your Role: You are an analytical assistant. Your task is to process a source image and a corresponding editing instruction, assuming the instruction accurately describes a desired transformation. You will 1) describe the source image, 2) output the editing instruction (potentially refined for clarity based on the source image context), and 3) describe the *imagined* result of applying that instruction.\n\nInput:\n1. Source Image: The original 'before' image.\n2. Source Instruction: A text instruction describing the edit to be performed on the Source Image. You *must assume* this instruction is accurate and feasible for the purpose of this task.\n\nTask Breakdown:\n1.  **Describe Source Image:** Generate a description (e.g., key subject, setting) of the Source Image by analyzing it. This will be the first line of your output.\n\n2.  **Output Editing Instruction:** This step determines the second line of your output.\n    * **Assumption:** The provided Source Instruction *accurately* describes the desired edit.\n    * **Goal:** Output a concise, single-line instruction based on the Source Instruction.\n    * **Refinement based on Source Image:** While the Source Instruction is assumed correct, analyze the Source Image to see if the instruction needs refinement for specificity. If the Source Image contains multiple similar objects and the Source Instruction is potentially ambiguous (e.g., \"change the car color\" when there are three cars), refine the instruction to be specific, using positional qualifiers (e.g., 'the left car', 'the bird on the top branch'), size ('the smaller dog', 'the largest building'), or other distinguishing visual features apparent in the Source Image. If the Source Instruction is already specific or if there's no ambiguity in the Source Image context, you can use it directly or with minor phrasing adjustments for naturalness. The *core meaning* of the Source Instruction must be preserved.\n    * **Output:** Present the resulting specific, single-line instruction as the second line.\n\n3.  **Describe Imagined Target Image:** Based *only* on the Source Image description (Line 1) and the Editing Instruction (Line 2), generate a description of the *imagined outcome*.\n    * Describe the scene from Line 1 *as if* the instruction from Line 2 has been successfully applied. Conceptualize the result of the edit on the source description.\n    * This description must be purely a logical prediction based on applying the instruction (Line 2) to the description in Line 1. Do *not* invent details not implied by the instruction or observed in the source image beyond the specified edit. This will be the third line of your output.\n\nOutput Format:\n* Your response *must* consist of exactly three lines.\n* Do not include any other explanations, comments, introductory phrases, labels (like \"Line 1:\"), or formatting.\n* Your output should be in English.\n\nLine 1: [Description of the Source Image]\nLine 2: [The specific, single-line editing instruction based on the Source Instruction and Source Image context]\nLine 3: [Description of the Imagined Target Image based on Lines 1 & 2]\n\nExample 1 (Instruction needs refinement):\nInput: (Source Image: Street with three parked cars), Source Instruction: \"change car color to red\"\nOutput:\nStreet with three parked cars.\nChange the color of the middle car to red.\nStreet with three parked cars, middle one is red.\n\nExample 2 (Instruction is specific enough):\nInput: (Source Image: Two brown dogs sitting on grass), Source Instruction: \"Add sunglasses to the dog on the left.\"\nOutput:\nTwo brown dogs sitting on grass.\nAdd sunglasses to the dog on the left.\nTwo brown dogs on grass, left one wearing sunglasses.\n\nNow, please generate the three-line output based on the Source Image and the Source Instruction: {source_instruction}",
            "text_2": "gemini-2.0-flash-001"
        }
    },
    {
        "id": 47,
        "name": "ShowText|pysssss",
        "prompts": {
            "text": "A man in a jacket and scarf stands with his hands in his pockets against a gray background.\nAdd glasses to this person.\nA man wearing glasses, a jacket, and a scarf stands with his hands in his pockets against a gray background."
        }
    },
    {
        "id": 48,
        "name": "LoraLoader",
        "prompts": {
            "text": "HiDream-E1-Full.safetensors"
        }
    },
    {
        "id": 49,
        "name": "CR Prompt Text",
        "prompts": {
            "text": "Your Role: You are an analytical assistant. Your task is to process a source image and a corresponding editing instruction, assuming the instruction accurately describes a desired transformation. You will 1) describe the source image, 2) output the editing instruction (potentially refined for clarity based on the source image context), and 3) describe the *imagined* result of applying that instruction.\n\nInput:\n1. Source Image: The original 'before' image.\n2. Source Instruction: xxxx\n\nTask Breakdown:\n1.  **Describe Source Image:** Generate a description (e.g., key subject, setting) of the Source Image by analyzing it. This will be the first line of your output.\n\n2.  **Output Editing Instruction:** This step determines the second line of your output.\n    * **Assumption:** The provided Source Instruction *accurately* describes the desired edit.\n    * **Goal:** Output a concise, single-line instruction based on the Source Instruction.\n    * **Refinement based on Source Image:** While the Source Instruction is assumed correct, analyze the Source Image to see if the instruction needs refinement for specificity. If the Source Image contains multiple similar objects and the Source Instruction is potentially ambiguous (e.g., \"change the car color\" when there are three cars), refine the instruction to be specific, using positional qualifiers (e.g., 'the left car', 'the bird on the top branch'), size ('the smaller dog', 'the largest building'), or other distinguishing visual features apparent in the Source Image. If the Source Instruction is already specific or if there's no ambiguity in the Source Image context, you can use it directly or with minor phrasing adjustments for naturalness. The *core meaning* of the Source Instruction must be preserved.\n    * **Output:** Present the resulting specific, single-line instruction as the second line.\n\n3.  **Describe Imagined Target Image:** Based *only* on the Source Image description (Line 1) and the Editing Instruction (Line 2), generate a description of the *imagined outcome*.\n    * Describe the scene from Line 1 *as if* the instruction from Line 2 has been successfully applied. Conceptualize the result of the edit on the source description.\n    * This description must be purely a logical prediction based on applying the instruction (Line 2) to the description in Line 1. Do *not* invent details not implied by the instruction or observed in the source image beyond the specified edit. This will be the third line of your output.\n\nOutput Format:\n* Your response *must* consist of exactly three lines.\n* Do not include any other explanations, comments, introductory phrases, labels (like \"Line 1:\"), or formatting.\n* Your output should be in English.\n\nLine 1: [Description of the Source Image]\nLine 2: [The specific, single-line editing instruction based on the Source Instruction and Source Image context]\nLine 3: [Description of the Imagined Target Image based on Lines 1 & 2]\n\nExample 1 (Instruction needs refinement):\nInput: (Source Image: Street with three parked cars), Source Instruction: \"change car color to red\"\nOutput:\nStreet with three parked cars.\nChange the color of the middle car to red.\nStreet with three parked cars, middle one is red.\n\nExample 2 (Instruction is specific enough):\nInput: (Source Image: Two brown dogs sitting on grass), Source Instruction: \"Add sunglasses to the dog on the left.\"\nOutput:\nTwo brown dogs sitting on grass.\nAdd sunglasses to the dog on the left.\nTwo brown dogs on grass, left one wearing sunglasses."
        }
    },
    {
        "id": 50,
        "name": "CR Text Replace"
    },
    {
        "id": 51,
        "name": "CR Prompt Text",
        "prompts": {
            "text": "add glassess to this person"
        }
    },
    {
        "id": 55,
        "name": "UnetLoaderGGUF"
    },
    {
        "id": 85,
        "name": "Note",
        "prompts": {
            "text": "keep it 768 for width"
        }
    },
    {
        "id": 87,
        "name": "CR Text Replace"
    },
    {
        "id": 92,
        "name": "easy cleanGpuUsed"
    },
    {
        "id": 93,
        "name": "CLIPTextEncode",
        "prompts": {
            "text": "low resolution, blur"
        }
    },
    {
        "id": 94,
        "name": "GeminiComfyUIStyler",
        "prompts": {
            "text": "artstyle-abstract"
        }
    },
    {
        "id": 96,
        "name": "MarkdownNote",
        "prompts": {
            "text": "# HiDream-E1 examples\n\nFirstly, you can use the auto restyle prompt:  \n**\"Apply \"_xyz_\" style to this image\"**  \nand choose your style from **GeminiComfyUIStyler**.\n\nOtherwise, see these example prompts:\n\n- Change the hair color to red  \n- Convert to Ghibli style  \n- Replace the apple with orange  \n- Change the wooden table to marble  \n- Add sunglasses  \n- Turn into sketch style  \n- Turn into Disney Pixar style  \n- Make the flame much larger  \n- Change the candle color to blue  \n- Write 'Save Our Planet' on the sign  \n- Write 'Level 30 Unlocked' on the cake  \n- Increase the saturation  \n- Remove the fork  \n- Put the dog in autumn leaves  \n- Change the style to a cubist painting  \n"
        }
    },
    {
        "id": 97,
        "name": "MarkdownNote",
        "prompts": {
            "text": "# \ud83e\udde0 ComfyUI-OllamaGemini Custom Node Setup Guide\n\nSet up the [ComfyUI-OllamaGemini](https://github.com/al-swaiti/ComfyUI-OllamaGemini/tree/dev) custom node to integrate with Gemini, OpenAI, Claude, and other LLMs.\n\n---\n\n## \ud83d\uddc2 Step 1: Clone the Repository\n\n### \ud83d\udd27 On Linux/macOS:\n\nOpen a terminal and run:\n\n```bash\ncd /path/to/ComfyUI/custom_nodes\ngit clone https://github.com/al-swaiti/ComfyUI-OllamaGemini.git\n```\n\n### \ud83e\ude9f On Windows:\n\nOpen **Command Prompt** or **PowerShell**, then run:\n\n```powershell\ncd path\\to\\ComfyUI\\custom_nodes\ngit clone https://github.com/al-swaiti/ComfyUI-OllamaGemini.git\n```\n\n---\n\n## \ud83d\udd10 Step 2: Get Your Gemini API Key\n\n1. Visit [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)\n2. Sign in with your Google account.\n3. Copy the **Gemini API Key**.\n\n---\n\n## \ud83d\udee0 Step 3: Configure Your API Keys\n\nOpen (or create) the following file:\n\n```\nComfyUI/custom_nodes/ComfyUI-OllamaGemini/config.json\n```\n\nPaste and edit the configuration like this:\n\n```json\n{\n  \"GEMINI_API_KEY\": \"your_gemini_api_key\",\n  \"OPENAI_API_KEY\": \"your_openai_api_key\",\n  \"ANTHROPIC_API_KEY\": \"your_claude_api_key\",\n  \"OLLAMA_URL\": \"http://localhost:11434\",\n  \"QWEN_API_KEY\": \"your_qwen_api_key\"\n}\n```\n\n\u2757 You may leave unused API keys as empty strings (`\"\"`) or delete those lines.\n\n---\n\n## \u2705 Final Step\n\nRestart ComfyUI. The new custom node should now be available and ready for use with Gemini and other models.\n\n---"
        }
    },
    {
        "id": 98,
        "name": "MarkdownNote",
        "prompts": {
            "text": "---\n\n### \ud83e\udde0 HiDream Model Resources\n\n---\n\n#### \ud83e\udd99 GGUF Models\n\nPrimary model: [HiDream (Civitai)](https://civitai.com/models/1553875)\nAvailable quantizations:\n\n* `fp8`\n* `Q3_K_L`\n* `Q4_K_S`\n\nOther GGUF variants:\n\ud83d\udd17 [HuggingFace - ND911 HiDream e1 full bf16 GGUFs](https://huggingface.co/ND911/HiDream_e1_full_bf16-ggufs/tree/main)\n\n---\n\n### \ud83c\udfa8 ComfyUI Integration (FP16)\n\nComfyUI-compatible model files:\n\ud83d\udd17 [Comfy-Org - HiDream-I1 for ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models)\n\n#### **\ud83d\udcc1 Recommended File Structure**\n\n```\n\ud83d\udcc2 ComfyUI/\n\u251c\u2500\u2500 \ud83d\udcc2 models/\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 text_encoders/\n\u2502   \u2502   \u251c\u2500\u2500 clip_l_hidream.safetensors\n\u2502   \u2502   \u251c\u2500\u2500 clip_g_hidream.safetensors\n\u2502   \u2502   \u251c\u2500\u2500 t5xxl_fp8_e4m3fn_scaled.safetensors\n\u2502   \u2502   \u2514\u2500\u2500 llama_3.1_8b_instruct_fp8_scaled.safetensors\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 vae/\n\u2502   \u2502   \u2514\u2500\u2500 ae.safetensors\n\u2502   \u251c\u2500\u2500 \ud83d\udcc2 diffusion_models/\n\u2502   \u2502   \u2514\u2500\u2500 hidream_e1_full_bf16.safetensors\n\u2502   \u2514\u2500\u2500 \ud83d\udcc2 lora/\n\u2502       \u2514\u2500\u2500 HiDream-E1-Full.safetensors  \u2190 Enhancement LoRA\n```\n\n---\n\n#### **Text Encoders**\n\n* [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n* [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)\n* [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors)\n* [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)\n\n#### **VAE**\n\n* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors)\n\n#### **Diffusion Model**\n\n* [hidream\\_e1\\_full\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors)\n\n#### **Enhancement LoRA**\n\n* [HiDream-E1-Full.safetensors](https://huggingface.co/HiDream-ai/HiDream-E1-Full/blob/main/HiDream-E1-Full.safetensors)\n  *(Place this in `ComfyUI/models/lora/`)*\n\n---\n"
        }
    },
    {
        "id": 99,
        "name": "Note",
        "prompts": {
            "text": "cfg image more > more close to image keep it between (2-4)"
        }
    }
]